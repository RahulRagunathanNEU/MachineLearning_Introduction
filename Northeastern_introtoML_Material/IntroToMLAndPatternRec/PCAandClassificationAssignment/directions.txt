This assignment was locked Nov 1, 2024 at 11:59pm.
1. (0 points) Please download the link file trainingData.csv Download trainingData.csvto your IDE.  This data set contains 10-D training data for class 1 and class 2. Your tasks will include performing dimensionality reduction,  classifier design and analysis.

2.  (5 points) Our first task is to reduce the dimension of the dataset, if possible.  Here, we use the notion of preserving scatter in doing so.  Our hope is that MOST scatter is due to class differences and PCA will retain that.  We begin by ignoring class labels, and find the per-column means and variances.  Center each column by its mean and divide by the square root of its variance, in that order. Since scatter is sensitive to units of measurement, this step removes that sensitivity.  You can find routines to do this automatically in your IDE by searching for "scaling".  

3.  (15 points) Perform PCA on the scaled dataset, again ignoring class labels, and produce a scree plot.  PCA produces a new set of 10 axes, ordered by the amount of scatter.  What percentage of scatter is contained in the first principal component?  The first two principal components?  Interpret your results.

4. (10 points) PCA produces the principal components as a part of its output.  Sometimes, these are called loadings.  Produce a bar-chart of the first principal component elements (10 of them).  Which components are negative and dominant?  Which are positive and dominant?  

5.  (15 points) A biplot is a useful visualization of the PCA output.  It displays the original, scaled  measurements M1,...M10 as 2-D vectors with axes given by the first two principal components.  Produce a biplot of your PCA output.  Which measurements have vector close in angle with each other?  Produce 4 such groups.  The original scaled measurements within each group are positively correlated.  (Positively correlated = measurements tend to go up together and down together.)

6.  (10 points) The length of each vector in the biplot help us understand its role in retaining scatter.  Original, scaled measurements with shortest vectors play the least role in retaining scatter.  The longest vectors indicate measurements that play the greatest role in retaining scatter.  Find the labels of 2 original, scales measurements which had the least scatter.  Find the labels of the 2 original, scaled measurements which had the most scatter.

7. (10 points) Two opposing vectors in the biplot indicate original, scaled measurements which are negatively correlated.  What is the measurement most negatively correlated with M4?

8. (0 point) Another useful output of PCA are scores.  Scores are the new coordinates of each original, scaled measurement in the new coordinate system (the principal components).  For example, the first score for each measurement represents the coordinate along the first principal component axis.  Attach two new columns to the data frame loaded in 1., called "score1" and "score2".  Our plan is to develop classifiers which only use these scores, not the original data, so what follows may seem familiar.

9.  (20 points) One the the benefits of the scores is that they are uncorrelated.   For Gaussian scores, this indicates independence.  Plot the two histograms of score1 on the same graph, one for each class.  Are those histograms bell-shaped, approximately?  The amount of overlap between  class histograms indicates the difficulty of good class decisions.  How is the overlap of these histograms?  Does the first score retain the class differences in the original measurerments?  Perform the same for score 2, and answer the above questions again.

10. (15 points) Using only score 1,  design the following classifiers for a single measurement: minimum risk, minimum probability of error, and Neyman-Pearson.  Please make use of the following data: P{class 1}=0.35, cost of saying class 1 when wrong = 1, cost of saying 2 when wrong = 20, false alarm probability <= 1/10000.  Plot the log likelihood ratio versus a range of scores.  On the same graph provide the log-likelihood ratio of the second score (even through we do not use it here.)  On the same graph, indicate the decision region for class 2 by horizontal lines at the log threshold heights.  Provide the risk of the minimum risk test, the probabilty of error of the minimum probability of error test, and false alarm probability and the power of the NP test.